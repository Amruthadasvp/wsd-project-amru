{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../source/dataloaders/\")\n",
    "sys.path.append(\"../source/models/\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"No tensorboardX package is found. Please install with the command: \\\n",
    "                        git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_format_utils as dfu\n",
    "from dataloaders import TrainValDataloader\n",
    "from bert import BertForWSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, Precision, Recall\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and process data\n",
    "#### Helper function to process dataset\n",
    "#### Sample Dataset has been preprocessed in order to match context in corpus (Semcor3) with proper gloss in Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process dataset with proper tokens, and embeddings.\n",
    "\n",
    "def gen_dataloader(_datapath,sample_size=100, batch_size=32, filter_bad_rows=True):\n",
    "    _df = pd.read_csv(_datapath)\n",
    "    \n",
    "    _smpldf = _df\n",
    "    if sample_size:\n",
    "        _smpldf = _df.sample(sample_size)\n",
    "    \n",
    "    dfu.tokenize_and_index(_smpldf)\n",
    "    dfu.gen_sentence_indexes(_smpldf)\n",
    "    dfu.find_index_of_target_token(_smpldf)\n",
    "    \n",
    "    if filter_bad_rows: # rows where the target word index exceeds tensor size \n",
    "        _smpldf = _smpldf[_smpldf.target_token_idx.apply(lambda x: x[0] <  dfu.MAX_LEN)]\n",
    "\n",
    "    _dl = TrainValDataloader(_smpldf,batch_size)\n",
    "    return _dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = gen_dataloader('../data/processed/sample_data.csv',sample_size=4000, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "### Declare optimizer classes and loss criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForWSD() \n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=2e-5)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch = (tens.to(device) for tens in batch)\n",
    "    b_tokens_tensor, b_sentence_tensor, b_target_token_tensor, y = batch\n",
    "    y_pred = model(b_tokens_tensor, b_sentence_tensor, b_target_token_tensor)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def eval_function(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = (tens.to(device) for tens in batch)\n",
    "        b_tokens_tensor, b_sentence_tensor, b_target_token_tensor, y = batch\n",
    "        y_pred = model(b_tokens_tensor, b_sentence_tensor, b_target_token_tensor)\n",
    "        return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Engine(process_function)\n",
    "train_evaluator = Engine(eval_function)\n",
    "validation_evaluator = Engine(eval_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(y_pred)\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Accuracy(output_transform=thresholded_output_transform).attach(train_evaluator, 'accuracy')\n",
    "Loss(criterion).attach(train_evaluator, 'bce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy(output_transform=thresholded_output_transform).attach(validation_evaluator, 'accuracy')\n",
    "Loss(criterion).attach(validation_evaluator, 'bce')\n",
    "\n",
    "precision = Precision(output_transform=thresholded_output_transform,average=True)\n",
    "recall = Recall(output_transform=thresholded_output_transform,average=True)\n",
    "\n",
    "\n",
    "precision.attach(validation_evaluator, 'Precision')\n",
    "recall.attach(validation_evaluator, 'Recall')\n",
    "F1 = (precision * recall * 2 / (precision + recall))\n",
    "F1.attach(validation_evaluator, 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, ['loss'])\n",
    "#pbar.attach(trainer, ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['bce']\n",
    "    return -val_loss\n",
    "\n",
    "handler = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\n",
    "validation_evaluator.add_event_handler(Events.COMPLETED, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer(model, data_loader, log_dir):\n",
    "    writer = SummaryWriter(logdir=log_dir)\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    batch = next(data_loader_iter)\n",
    "    batch = tuple(b.to(device) for b in batch)[:-1]\n",
    "    try:\n",
    "        writer.add_graph(model, batch)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save model graph: {}\".format(e))\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs'\n",
    "writer = create_summary_writer(model, dl.train_dataloader, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Result logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_interval = 10\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dl.train_dataloader) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        #print(\"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "        #      \"\".format(engine.state.epoch, iter, len(dl.train_dataloader), engine.state.output))\n",
    "        writer.add_scalar(\"training/loss\", engine.state.output, engine.state.iteration)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    train_evaluator.run(dl.train_dataloader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_bce = metrics['bce']\n",
    "    pbar.log_message(\n",
    "        \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "        .format(engine.state.epoch, avg_accuracy, avg_bce))\n",
    "    writer.add_scalar(\"training/avg_loss\", avg_accuracy, engine.state.epoch)\n",
    "    writer.add_scalar(\"training/avg_accuracy\", avg_bce, engine.state.epoch)\n",
    "    \n",
    "def log_validation_results(engine):\n",
    "    validation_evaluator.run(dl.val_dataloader)\n",
    "    metrics = validation_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_bce = metrics['bce']\n",
    "    avg_precision = metrics['Precision']\n",
    "    avg_recall = metrics['Recall']\n",
    "    avg_F1 = metrics['F1']\n",
    "    pbar.log_message(\n",
    "        \"Validation Results - Epoch: {} Averages: Acc: {:.3f} Loss: {:.3f} Precision: {:.3f} Recall: {:.3f} F1: {:.3f}\"\n",
    "        .format(engine.state.epoch, avg_accuracy, avg_bce, avg_precision, avg_recall, avg_F1))\n",
    "    pbar.n = pbar.last_print_n = 0\n",
    "    writer.add_scalar(\"valdation/avg_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "    writer.add_scalar(\"valdation/avg_loss\", avg_bce, engine.state.epoch)\n",
    "    writer.add_scalar(\"valdation/avg_F1\", avg_F1, engine.state.epoch)\n",
    "    writer.add_scalar(\"valdation/avg_precision\", avg_precision, engine.state.epoch)\n",
    "    writer.add_scalar(\"valdation/avg_recall\", avg_recall, engine.state.epoch)\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('./model_checkpoints/models', 'bertWSD', save_interval=1, n_saved=2, \n",
    "                               create_dir=True, save_as_state_dict=True,require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'bertWSD': model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(dl.train_dataloader, max_epochs=3)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels using scikits learn\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def accuracy_precision_recall_fscore(confusion_matrix):\n",
    "    #TP,FP,FN,TN = confusion_matrix.ravel()\n",
    "    TN, FP, FN, TP = confusion_matrix.ravel()\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "    precision = TP/(TP+FP) \n",
    "    recall = TP/(TP+FN) \n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    return accuracy,precision,recall,F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
