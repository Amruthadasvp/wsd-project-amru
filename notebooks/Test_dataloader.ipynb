{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext.data import Field, Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../source/dataloaders/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import data_format_utils as dfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/preprocessed/semcor_debug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "tokenize = lambda x: DEF_TOKENIZER(x)\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]\n",
    "def find_token(_tokenized,_target):\n",
    "    idxs_token  = [i for i,word  in \\\n",
    "                         enumerate(_tokenized) \\\n",
    "                         if word == _target.lower()]\n",
    "    return idxs_token[0]\n",
    "\n",
    "def get_index_of_sep(_token_sent):\n",
    "        \"\"\"\n",
    "        Get index of sep token and generate sentence index array\n",
    "        \"\"\" \n",
    "        _index_sep_tokens = [i for i,word  in enumerate(_token_sent) \\\n",
    "                           if word == '[SEP]']\n",
    "        _sentence_indexes = np.array([0]*(_index_sep_tokens[0]+1)\\\n",
    "                                     +[1]*(_index_sep_tokens[1]-_index_sep_tokens[0]))\n",
    "        return _sentence_indexes\n",
    "\n",
    "\n",
    "def tranform_row(_row):\n",
    "    sent = dfu.format_sentences_BERT(_row)\n",
    "    tok_sent = DEF_TOKENIZER.tokenize(sent)\n",
    "    target_word_tokens = DEF_TOKENIZER.tokenize(_row.target_word)\n",
    "    idx = find_token(tok_sent,target_word_tokens[0])\n",
    "    idx_sent = DEF_TOKENIZER.convert_tokens_to_ids(tok_sent)\n",
    "    sentence_embed = get_index_of_sep(tok_sent)\n",
    "    return idx_sent, sentence_embed, idx\n",
    "    \n",
    "    \n",
    "    \n",
    "out = tranform_row(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "class CorpusTransformDataset(Dataset):\n",
    "    \"\"\"\n",
    "    pytorch dataset handling class    \n",
    "    \"\"\"\n",
    "    def __init__(self, data,pad_len=MAX_LEN):\n",
    "        self.corpus_dataframe = data\n",
    "        self.pad_len = pad_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_dataframe.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_row = self.corpus_dataframe.iloc[idx]\n",
    "        token_idxs_raw,sentence_embed_raw,target_token_idx = tranform_row(raw_row)\n",
    "        token_idxs_raw = torch.tensor(token_idxs_raw)\n",
    "        sentence_embed_raw = torch.tensor(sentence_embed_raw, dtype=torch.int64)\n",
    "\n",
    "        if token_idxs_raw.shape[0] >= self.pad_len:\n",
    "            token_idxs_pad = token_idxs_raw[:self.pad_len]\n",
    "            sentence_embed_pad = sentence_embed_raw[:self.pad_len]\n",
    "        else: # pad with 0s and 1s\n",
    "            token_idxs_pad = F.pad(token_idxs_raw,(0,self.pad_len-token_idxs_raw.shape[0]))\n",
    "            sentence_embed_pad = F.pad(sentence_embed_raw,(0,self.pad_len-sentence_embed_raw.shape[0]),value=1)\n",
    "        \n",
    "        return (token_idxs_pad,  # Input token encodings,\n",
    "                sentence_embed_pad, # Sentence encoding\n",
    "                torch.tensor(target_token_idx, dtype=torch.int64), # Target token indexes\n",
    "                torch.tensor(raw_row['is_proper_gloss'],dtype=torch.int64)) # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2006,  1035,  1996,  1035,  2060,  1035,  2192,  1037,  3129,\n",
       "         2040,  2467,  2038,  2042, 21813,  1998, 20865,  3512,  2089,  3402,\n",
       "         2468, 13135,  4851, 25428,  2360,  2005,  2128, 12054, 25863,  2008,\n",
       "         2010,  2564,  2145,  4858,  2032, 16166,   102,  2191,  1037,  5456,\n",
       "         1010,  2191,  1037,  2047,  4531,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CorpusTransformDataset(df,pad_len=128)\n",
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2006])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = train_dataset[0]\n",
    "out[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "padd = F.pad(out[0],(0,128-out[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize_and_index() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d6fb0d1973e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train_dataset[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m dfu.tokenize_and_index(df,output_len=128,weak_supervision=True,\n\u001b[0;32m----> 3\u001b[0;31m                        tokenizer=DEF_TOKENIZER,verbose=False)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tokenize_and_index() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "#train_dataset[0]\n",
    "dfu.tokenize_and_index(df,output_len=128,weak_supervision=True,\n",
    "                       tokenizer=DEF_TOKENIZER,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function data_format_utils.format_sentences_BERT(_row, weak_supervision=False)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu.format_sentences_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.functional' has no attribute 'pad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-652d30dc6364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.functional' has no attribute 'pad'"
     ]
    }
   ],
   "source": [
    "F.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
